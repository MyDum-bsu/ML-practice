{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = None # length of the input sequences of integers\n",
    "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram(nn.Module):\n",
    "    \"\"\"\n",
    "    Bigram Language Model 'neural net', simply a lookup table of logits for the\n",
    "    next character given a previous character.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n = config.vocab_size\n",
    "        self.logits = nn.Parameter(torch.zeros((n, n)))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return 1 # this model only needs one previous character to predict the next\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "         # 'forward pass', lol\n",
    "        logits = self.logits[idx]\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, words, chars, max_word_length):\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        self.max_word_length = max_word_length\n",
    "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def get_output_length(self):\n",
    "        return self.max_word_length + 1 # <START> token followed by words\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        x[1:1+len(ix)] = ix\n",
    "        y[:len(ix)] = ix\n",
    "        y[len(ix)+1:] = -1 # index -1 will mask the loss at the inactive locations\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(input_file):\n",
    "\n",
    "    # preprocessing of the input text file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    words = data.splitlines()\n",
    "    words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "    words = [w for w in words if w] # get rid of any empty strings\n",
    "    chars = sorted(list(set(''.join(words)))) # all the possible characters\n",
    "    max_word_length = max(len(w) for w in words)\n",
    "    print(f\"number of examples in the dataset: {len(words)}\")\n",
    "    print(f\"max word length: {max_word_length}\")\n",
    "    print(f\"number of unique characters in the vocabulary: {len(chars)}\")\n",
    "    print(\"vocabulary:\")\n",
    "    print(''.join(chars))\n",
    "\n",
    "    # partition the input data into a training and the test set\n",
    "    test_set_size = min(1000, int(len(words) * 0.1)) # 10% of the training set, or up to 1000 examples\n",
    "    rp = torch.randperm(len(words)).tolist()\n",
    "    train_words = [words[i] for i in rp[:-test_set_size]]\n",
    "    test_words = [words[i] for i in rp[-test_set_size:]]\n",
    "    print(f\"split up the dataset into {len(train_words)} training examples and {len(test_words)} test examples\")\n",
    "\n",
    "    # wrap in dataset objects\n",
    "    train_dataset = CharDataset(train_words, chars, max_word_length)\n",
    "    test_dataset = CharDataset(test_words, chars, max_word_length)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
    "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
    "        self.data_iter = iter(self.train_loader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            batch = next(self.data_iter)\n",
    "        except StopIteration: # this will technically only happen after 1e10 samples (i.e. basically never)\n",
    "            self.data_iter = iter(self.train_loader)\n",
    "            batch = next(self.data_iter)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, device, dataset, batch_size=50, max_batches=None):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        X, Y = batch\n",
    "        _, loss = model(X, Y)\n",
    "        losses.append(loss.item())\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "    mean_loss = torch.tensor(losses).mean().item()\n",
    "    model.train() # reset model back to training mode\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "\n",
    "    block_size = model.get_block_size()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, _ = model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(model, train_dataset, test_dataset, device, top_k, num=10):\n",
    "    \"\"\" samples from the model and pretty prints the decoded samples \"\"\"\n",
    "    X_init = torch.zeros(num, 1, dtype=torch.long).to(device)\n",
    "    top_k = top_k if top_k != -1 else None\n",
    "    steps = train_dataset.get_output_length() - 1 # -1 because we already start with <START> token (index 0)\n",
    "    X_samp = generate(model, X_init, steps, top_k=top_k, do_sample=True).to('cpu')\n",
    "    train_samples, test_samples, new_samples = [], [], []\n",
    "    for i in range(X_samp.size(0)):\n",
    "        # get the i'th row of sampled integers, as python list\n",
    "        row = X_samp[i, 1:].tolist() # note: we need to crop out the first <START> token\n",
    "        # token 0 is the <STOP> token, so we crop the output sequence at that point\n",
    "        crop_index = row.index(0) if 0 in row else len(row)\n",
    "        row = row[:crop_index]\n",
    "        word_samp = train_dataset.decode(row)\n",
    "        # separately track samples that we have and have not seen before\n",
    "        if train_dataset.contains(word_samp):\n",
    "            train_samples.append(word_samp)\n",
    "        elif test_dataset.contains(word_samp):\n",
    "            test_samples.append(word_samp)\n",
    "        else:\n",
    "            new_samples.append(word_samp)\n",
    "    print('-'*80)\n",
    "    for lst, desc in [(train_samples, 'in train'), (test_samples, 'in test'), (new_samples, 'new')]:\n",
    "        print(f\"{len(lst)} samples that are {desc}:\")\n",
    "        for word in lst:\n",
    "            print(word)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 64\n",
    "n_embd2 = 64\n",
    "\n",
    "max_steps = 1000\n",
    "num_workers = 4\n",
    "batch_size = 32\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.01\n",
    "top_k = -1\n",
    "work_dir = 'out'\n",
    "input_file = 'names.txt'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in the dataset: 32033\n",
      "max word length: 15\n",
      "number of unique characters in the vocabulary: 26\n",
      "vocabulary:\n",
      "abcdefghijklmnopqrstuvwxyz\n",
      "split up the dataset into 31033 training examples and 1000 test examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_datasets(input_file)\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "block_size = train_dataset.get_output_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(vocab_size=vocab_size, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bigram(config)\n",
    "writer = SummaryWriter(log_dir=work_dir)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                              weight_decay=weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
    "batch_loader = InfiniteDataLoader(train_dataset, batch_size=batch_size,\n",
    "                                  pin_memory=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 3.2958 | step time 33.74ms\n",
      "step 10 | loss 3.2918 | step time 2.64ms\n",
      "step 20 | loss 3.2881 | step time 2.59ms\n",
      "step 30 | loss 3.2835 | step time 2.50ms\n",
      "step 40 | loss 3.2807 | step time 2.48ms\n",
      "step 50 | loss 3.2754 | step time 2.46ms\n",
      "step 60 | loss 3.2732 | step time 2.43ms\n",
      "step 70 | loss 3.2676 | step time 4.14ms\n",
      "step 80 | loss 3.2658 | step time 2.49ms\n",
      "step 90 | loss 3.2610 | step time 2.28ms\n",
      "step 100 | loss 3.2568 | step time 2.26ms\n",
      "step 110 | loss 3.2525 | step time 2.34ms\n",
      "step 120 | loss 3.2515 | step time 2.32ms\n",
      "step 130 | loss 3.2462 | step time 2.21ms\n",
      "step 140 | loss 3.2411 | step time 4.09ms\n",
      "step 150 | loss 3.2377 | step time 2.23ms\n",
      "step 160 | loss 3.2386 | step time 2.32ms\n",
      "step 170 | loss 3.2330 | step time 2.27ms\n",
      "step 180 | loss 3.2239 | step time 2.49ms\n",
      "step 190 | loss 3.2276 | step time 2.37ms\n",
      "step 200 | loss 3.2191 | step time 2.44ms\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "gk\n",
      "blghyrayzpjqsua\n",
      "barcahcckveeetb\n",
      "jyxbl\n",
      "akmuduxgsgbrshp\n",
      "qvdzhveqealygsp\n",
      "sgqgdbyjxch\n",
      "mzdsrowi\n",
      "xlafnjkjs\n",
      "vt\n",
      "--------------------------------------------------------------------------------\n",
      "step 210 | loss 3.2192 | step time 2.39ms\n",
      "step 220 | loss 3.2105 | step time 2.46ms\n",
      "step 230 | loss 3.2084 | step time 5.88ms\n",
      "step 240 | loss 3.2068 | step time 4.65ms\n",
      "step 250 | loss 3.2054 | step time 10.47ms\n",
      "step 260 | loss 3.1971 | step time 7.64ms\n",
      "step 270 | loss 3.1990 | step time 2.56ms\n",
      "step 280 | loss 3.1929 | step time 2.36ms\n",
      "step 290 | loss 3.1832 | step time 2.33ms\n",
      "step 300 | loss 3.1739 | step time 2.83ms\n",
      "step 310 | loss 3.1845 | step time 2.22ms\n",
      "step 320 | loss 3.1676 | step time 2.29ms\n",
      "step 330 | loss 3.1706 | step time 2.21ms\n",
      "step 340 | loss 3.1606 | step time 2.15ms\n",
      "step 350 | loss 3.1749 | step time 2.26ms\n",
      "step 360 | loss 3.1549 | step time 4.95ms\n",
      "step 370 | loss 3.1576 | step time 2.18ms\n",
      "step 380 | loss 3.1536 | step time 2.49ms\n",
      "step 390 | loss 3.1531 | step time 8.55ms\n",
      "step 400 | loss 3.1450 | step time 3.50ms\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "e\n",
      "pevdbwi\n",
      "zansznnovgkzefr\n",
      "\n",
      "qxbvktesnipj\n",
      "mmcxdbn\n",
      "kmwuurcqfbdzfur\n",
      "sffrxrtvxarxboj\n",
      "ftzzynjonciwdeg\n",
      "wjinncmfvfyqeqq\n",
      "--------------------------------------------------------------------------------\n",
      "step 410 | loss 3.1541 | step time 2.25ms\n",
      "step 420 | loss 3.1474 | step time 2.36ms\n",
      "step 430 | loss 3.1459 | step time 9.84ms\n",
      "step 440 | loss 3.1514 | step time 2.23ms\n",
      "step 450 | loss 3.1417 | step time 2.34ms\n",
      "step 460 | loss 3.1386 | step time 2.25ms\n",
      "step 470 | loss 3.1266 | step time 10.24ms\n",
      "step 480 | loss 3.1217 | step time 2.37ms\n",
      "step 490 | loss 3.1210 | step time 2.39ms\n",
      "step 500 | loss 3.1245 | step time 6.09ms\n",
      "step 500 train loss: 3.1191954612731934 test loss: 3.116238832473755\n",
      "test loss 3.116238832473755 is the best so far, saving model to out/model.pt\n",
      "step 510 | loss 3.1253 | step time 2.42ms\n",
      "step 520 | loss 3.1099 | step time 2.43ms\n",
      "step 530 | loss 3.1119 | step time 2.53ms\n",
      "step 540 | loss 3.0999 | step time 10.15ms\n",
      "step 550 | loss 3.0935 | step time 5.35ms\n",
      "step 560 | loss 3.0948 | step time 2.87ms\n",
      "step 570 | loss 3.1041 | step time 5.87ms\n",
      "step 580 | loss 3.0842 | step time 6.74ms\n",
      "step 590 | loss 3.0942 | step time 2.13ms\n",
      "step 600 | loss 3.0708 | step time 2.25ms\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "vfwjzzphncvthyq\n",
      "bbwlomkhlzhqkqw\n",
      "we\n",
      "bgonnaohbgmduws\n",
      "rfsaobszbettjdm\n",
      "dpdcgeaiovulgpv\n",
      "giyipvfecmaltty\n",
      "aojffxyn\n",
      "mmaqnnfcexjhkio\n",
      "chyiklktlq\n",
      "--------------------------------------------------------------------------------\n",
      "step 610 | loss 3.0740 | step time 4.01ms\n",
      "step 620 | loss 3.0748 | step time 3.07ms\n",
      "step 630 | loss 3.0899 | step time 2.18ms\n",
      "step 640 | loss 3.0879 | step time 2.39ms\n",
      "step 650 | loss 3.0810 | step time 3.66ms\n",
      "step 660 | loss 3.0703 | step time 2.32ms\n",
      "step 670 | loss 3.0722 | step time 8.02ms\n",
      "step 680 | loss 3.0669 | step time 2.47ms\n",
      "step 690 | loss 3.0533 | step time 2.89ms\n",
      "step 700 | loss 3.0420 | step time 6.75ms\n",
      "step 710 | loss 3.0415 | step time 4.73ms\n",
      "step 720 | loss 3.0612 | step time 2.10ms\n",
      "step 730 | loss 3.0515 | step time 3.20ms\n",
      "step 740 | loss 3.0278 | step time 2.12ms\n",
      "step 750 | loss 3.0221 | step time 4.41ms\n",
      "step 760 | loss 3.0280 | step time 4.24ms\n",
      "step 770 | loss 3.0519 | step time 2.19ms\n",
      "step 780 | loss 3.0260 | step time 2.26ms\n",
      "step 790 | loss 3.0215 | step time 2.27ms\n",
      "step 800 | loss 2.9970 | step time 2.46ms\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "sikotvnojeerbjj\n",
      "bwqujhinphlylvn\n",
      "qscdgkwbtjeyqjj\n",
      "novptio\n",
      "krokudeajqyainv\n",
      "y\n",
      "khkuykmav\n",
      "rbzk\n",
      "fusefqyuhtzjwxk\n",
      "ejihyahnhqftsa\n",
      "--------------------------------------------------------------------------------\n",
      "step 810 | loss 3.0393 | step time 2.35ms\n",
      "step 820 | loss 3.0014 | step time 2.13ms\n",
      "step 830 | loss 2.9951 | step time 2.41ms\n",
      "step 840 | loss 2.9878 | step time 4.19ms\n",
      "step 850 | loss 3.0200 | step time 2.25ms\n",
      "step 860 | loss 3.0034 | step time 2.37ms\n",
      "step 870 | loss 3.0199 | step time 2.25ms\n",
      "step 880 | loss 2.9986 | step time 2.40ms\n",
      "step 890 | loss 2.9894 | step time 4.03ms\n",
      "step 900 | loss 3.0118 | step time 2.41ms\n",
      "step 910 | loss 2.9841 | step time 14.56ms\n",
      "step 920 | loss 3.0064 | step time 2.17ms\n",
      "step 930 | loss 2.9797 | step time 2.16ms\n",
      "step 940 | loss 3.0083 | step time 2.46ms\n",
      "step 950 | loss 2.9818 | step time 2.16ms\n",
      "step 960 | loss 2.9852 | step time 2.47ms\n",
      "step 970 | loss 2.9734 | step time 2.12ms\n",
      "step 980 | loss 2.9715 | step time 2.28ms\n",
      "step 990 | loss 2.9566 | step time 2.25ms\n"
     ]
    }
   ],
   "source": [
    "best_loss = None\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # get the next batch, ship to device, and unpack it to input and target\n",
    "    batch = batch_loader.next()\n",
    "    batch = [t.to(device) for t in batch]\n",
    "    X, Y = batch\n",
    "\n",
    "    # feed into the model\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    # calculate the gradient, update the weights\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # logging\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
    "\n",
    "    # evaluate the model\n",
    "    if step > 0 and step % 500 == 0:\n",
    "        train_loss = evaluate(model, device, train_dataset, batch_size=100, max_batches=10)\n",
    "        test_loss  = evaluate(model, device, test_dataset,  batch_size=100, max_batches=10)\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, step)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, step)\n",
    "        writer.flush()\n",
    "        print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n",
    "        # save the model to disk if it has improved\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            out_path = os.path.join(work_dir, \"model.pt\")\n",
    "            print(f\"test loss {test_loss} is the best so far, saving model to {out_path}\")\n",
    "            torch.save(model.state_dict(), out_path)\n",
    "            best_loss = test_loss\n",
    "\n",
    "    # sample from the model\n",
    "    if step > 0 and step % 200 == 0:\n",
    "        print_samples(model, train_dataset, test_dataset,device, top_k, num=10)\n",
    "\n",
    "    step += 1\n",
    "    # termination conditions\n",
    "    if max_steps >= 0 and step >= max_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "23 samples that are new:\n",
      "sqishqqmiatlera\n",
      "qjddrgdbiwkywiz\n",
      "lasvmbrbvhiudin\n",
      "vad\n",
      "khjuuoxgokwfwka\n",
      "naosu\n",
      "liaucke\n",
      "keciarkbrhdjxsn\n",
      "dor\n",
      "iustshwdyp\n",
      "tzkeorcjarynw\n",
      "tykmqmmsbw\n",
      "f\n",
      "harisszokajies\n",
      "eralvqcbhxcr\n",
      "uvqqfenihbltljk\n",
      "fff\n",
      "wfthvufkpydeiog\n",
      "pbrehnfchuhfvje\n",
      "\n",
      "jtexgrtuhyb\n",
      "mzr\n",
      "raihvwkfjalegyl\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_samples(model, train_dataset, test_dataset,device, top_k, num=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
