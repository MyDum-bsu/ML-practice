{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = None # length of the input sequences of integers\n",
    "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram(nn.Module):\n",
    "    \"\"\"\n",
    "    Bigram Language Model 'neural net', simply a lookup table of logits for the\n",
    "    next character given a previous character.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n = config.vocab_size\n",
    "        self.logits = nn.Parameter(torch.zeros((n, n)))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return 1 # this model only needs one previous character to predict the next\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "         # 'forward pass', lol\n",
    "        logits = self.logits[idx]\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.block_size * config.n_embd, config.n_embd2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        )\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        embs = []\n",
    "        for _ in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "            embs.append(tok_emb)\n",
    "\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, words, chars, max_word_length):\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        self.max_word_length = max_word_length\n",
    "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def get_output_length(self):\n",
    "        return self.max_word_length + 1 # <START> token followed by words\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        x[1:1+len(ix)] = ix\n",
    "        y[:len(ix)] = ix\n",
    "        y[len(ix)+1:] = -1 # index -1 will mask the loss at the inactive locations\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(input_file):\n",
    "\n",
    "    # preprocessing of the input text file\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    words = data.splitlines()\n",
    "    words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "    words = [w for w in words if w] # get rid of any empty strings\n",
    "    chars = sorted(list(set(''.join(words)))) # all the possible characters\n",
    "    max_word_length = max(len(w) for w in words)\n",
    "    print(f\"number of examples in the dataset: {len(words)}\")\n",
    "    print(f\"max word length: {max_word_length}\")\n",
    "    print(f\"number of unique characters in the vocabulary: {len(chars)}\")\n",
    "    print(\"vocabulary:\")\n",
    "    print(''.join(chars))\n",
    "\n",
    "    # partition the input data into a training and the test set\n",
    "    test_set_size = min(1000, int(len(words) * 0.1)) # 10% of the training set, or up to 1000 examples\n",
    "    rp = torch.randperm(len(words)).tolist()\n",
    "    train_words = [words[i] for i in rp[:-test_set_size]]\n",
    "    test_words = [words[i] for i in rp[-test_set_size:]]\n",
    "    print(f\"split up the dataset into {len(train_words)} training examples and {len(test_words)} test examples\")\n",
    "\n",
    "    # wrap in dataset objects\n",
    "    train_dataset = CharDataset(train_words, chars, max_word_length)\n",
    "    test_dataset = CharDataset(test_words, chars, max_word_length)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
    "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
    "        self.data_iter = iter(self.train_loader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            batch = next(self.data_iter)\n",
    "        except StopIteration: # this will technically only happen after 1e10 samples (i.e. basically never)\n",
    "            self.data_iter = iter(self.train_loader)\n",
    "            batch = next(self.data_iter)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, device, dataset, batch_size=50, max_batches=None):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        X, Y = batch\n",
    "        _, loss = model(X, Y)\n",
    "        losses.append(loss.item())\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "    mean_loss = torch.tensor(losses).mean().item()\n",
    "    model.train() # reset model back to training mode\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "\n",
    "    block_size = model.get_block_size()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, _ = model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(model, train_dataset, test_dataset, device, top_k, num=10):\n",
    "    \"\"\" samples from the model and pretty prints the decoded samples \"\"\"\n",
    "    X_init = torch.zeros(num, 1, dtype=torch.long).to(device)\n",
    "    top_k = top_k if top_k != -1 else None\n",
    "    steps = train_dataset.get_output_length() - 1 # -1 because we already start with <START> token (index 0)\n",
    "    X_samp = generate(model, X_init, steps, top_k=top_k, do_sample=True).to('cpu')\n",
    "    train_samples, test_samples, new_samples = [], [], []\n",
    "    for i in range(X_samp.size(0)):\n",
    "        # get the i'th row of sampled integers, as python list\n",
    "        row = X_samp[i, 1:].tolist() # note: we need to crop out the first <START> token\n",
    "        # token 0 is the <STOP> token, so we crop the output sequence at that point\n",
    "        crop_index = row.index(0) if 0 in row else len(row)\n",
    "        row = row[:crop_index]\n",
    "        word_samp = train_dataset.decode(row)\n",
    "        # separately track samples that we have and have not seen before\n",
    "        if train_dataset.contains(word_samp):\n",
    "            train_samples.append(word_samp)\n",
    "        elif test_dataset.contains(word_samp):\n",
    "            test_samples.append(word_samp)\n",
    "        else:\n",
    "            new_samples.append(word_samp)\n",
    "    print('-'*80)\n",
    "    for lst, desc in [(train_samples, 'in train'), (test_samples, 'in test'), (new_samples, 'new')]:\n",
    "        print(f\"{len(lst)} samples that are {desc}:\")\n",
    "        for word in lst:\n",
    "            print(word)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 64\n",
    "n_embd2 = 64\n",
    "\n",
    "max_steps = 1000\n",
    "num_workers = 4\n",
    "batch_size = 32\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.01\n",
    "top_k = -1\n",
    "work_dir = 'out'\n",
    "input_file = 'names.txt'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in the dataset: 32033\n",
      "max word length: 15\n",
      "number of unique characters in the vocabulary: 26\n",
      "vocabulary:\n",
      "abcdefghijklmnopqrstuvwxyz\n",
      "split up the dataset into 31033 training examples and 1000 test examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_datasets(input_file)\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "block_size = train_dataset.get_output_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(vocab_size=vocab_size, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Bigram(config)\n",
    "model = MLP(config)\n",
    "writer = SummaryWriter(log_dir=work_dir)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                              weight_decay=weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
    "batch_loader = InfiniteDataLoader(train_dataset, batch_size=batch_size,\n",
    "                                  pin_memory=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    best_loss = None\n",
    "    step = 0\n",
    "    while True:\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # get the next batch, ship to device, and unpack it to input and target\n",
    "        batch = batch_loader.next()\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        X, Y = batch\n",
    "\n",
    "        # feed into the model\n",
    "        logits, loss = model(X, Y)\n",
    "\n",
    "        # calculate the gradient, update the weights\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # logging\n",
    "        if step % 10 == 0:\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
    "\n",
    "        # evaluate the model\n",
    "        if step > 0 and step % 500 == 0:\n",
    "            train_loss = evaluate(model, device, train_dataset, batch_size=100, max_batches=10)\n",
    "            test_loss  = evaluate(model, device, test_dataset,  batch_size=100, max_batches=10)\n",
    "            writer.add_scalar(\"Loss/train\", train_loss, step)\n",
    "            writer.add_scalar(\"Loss/test\", test_loss, step)\n",
    "            writer.flush()\n",
    "            print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n",
    "            # save the model to disk if it has improved\n",
    "            if best_loss is None or test_loss < best_loss:\n",
    "                out_path = os.path.join(work_dir, \"model.pt\")\n",
    "                print(f\"test loss {test_loss} is the best so far, saving model to {out_path}\")\n",
    "                torch.save(model.state_dict(), out_path)\n",
    "                best_loss = test_loss\n",
    "\n",
    "        # sample from the model\n",
    "        if step > 0 and step % 200 == 0:\n",
    "            print_samples(model, train_dataset, test_dataset,device, top_k, num=10)\n",
    "\n",
    "        step += 1\n",
    "        # termination conditions\n",
    "        if max_steps >= 0 and step >= max_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "2 samples that are in train:\n",
      "paris\n",
      "amaria\n",
      "0 samples that are in test:\n",
      "21 samples that are new:\n",
      "aleynk\n",
      "orania\n",
      "seithyan\n",
      "leliye\n",
      "kanaiah\n",
      "amisliela\n",
      "zebyn\n",
      "eeic\n",
      "bresoh\n",
      "svaniki\n",
      "yaray\n",
      "dutleu\n",
      "agwilina\n",
      "earmiah\n",
      "soney\n",
      "loifran\n",
      "gamriylen\n",
      "facelsea\n",
      "horrajven\n",
      "khis\n",
      "deetri\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_samples(model, train_dataset, test_dataset,device, top_k, num=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.xh_to_h = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "\n",
    "    def forward(self, xt, hprev):\n",
    "        xh = torch.cat([xt, hprev], dim=1)\n",
    "        ht = torch.tanh(self.xh_to_h(xh))\n",
    "        return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.xh_to_z = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.xh_to_r = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.xh_to_hbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "\n",
    "    def forward(self, xt, hprev):\n",
    "        xh = torch.cat([xt, hprev], dim=1)\n",
    "        r = F.sigmoid(self.xh_to_r(xh))\n",
    "        hprev_reset = r * hprev\n",
    "\n",
    "        xhr = torch.cat([xt, hprev_reset], dim=1)\n",
    "        hbar = F.tanh(self.xh_to_hbar(xhr))\n",
    "        z = F.sigmoid(self.xh_to_z(xh))\n",
    "        ht = (1-z) * hprev + z * hbar\n",
    "        return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, config, cell_type):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2))\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.cell = {'rnn': RNNCell(config),\n",
    "                     'gru': GRUCell(config),}[cell_type]\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        emb = self.wte(idx)\n",
    "        hprev = self.start.expand((b, -1))\n",
    "\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            ht = self.cell(xt, hprev) # (b, n_embd2)\n",
    "            hprev = ht\n",
    "            hiddens.append(ht)\n",
    "        \n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(config, cell_type='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 3.3183 | step time 65.31ms\n",
      "step 10 | loss 3.2998 | step time 9.99ms\n",
      "step 20 | loss 3.3203 | step time 12.77ms\n",
      "step 30 | loss 3.2985 | step time 10.09ms\n",
      "step 40 | loss 3.3008 | step time 11.19ms\n",
      "step 50 | loss 3.3174 | step time 10.54ms\n",
      "step 60 | loss 3.3099 | step time 13.27ms\n",
      "step 70 | loss 3.3073 | step time 11.38ms\n",
      "step 80 | loss 3.3109 | step time 13.81ms\n",
      "step 90 | loss 3.3230 | step time 13.08ms\n",
      "step 100 | loss 3.3222 | step time 14.99ms\n",
      "step 110 | loss 3.3309 | step time 9.35ms\n",
      "step 120 | loss 3.3019 | step time 14.79ms\n",
      "step 130 | loss 3.3192 | step time 11.60ms\n",
      "step 140 | loss 3.2831 | step time 21.01ms\n",
      "step 150 | loss 3.3243 | step time 14.74ms\n",
      "step 160 | loss 3.3263 | step time 15.64ms\n",
      "step 170 | loss 3.3269 | step time 13.45ms\n",
      "step 180 | loss 3.3081 | step time 10.62ms\n",
      "step 190 | loss 3.2972 | step time 16.14ms\n",
      "step 200 | loss 3.3235 | step time 16.97ms\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "gkxoifwbtuyigmz\n",
      "fkdkqdhd\n",
      "uadckliz\n",
      "zjqm\n",
      "m\n",
      "opj\n",
      "dquxxaqwchilsmz\n",
      "nnqhi\n",
      "xgcxmzpctpendse\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "step 210 | loss 3.3262 | step time 13.54ms\n",
      "step 220 | loss 3.3222 | step time 12.25ms\n",
      "step 230 | loss 3.3061 | step time 9.27ms\n",
      "step 240 | loss 3.3092 | step time 14.38ms\n",
      "step 250 | loss 3.3114 | step time 14.13ms\n",
      "step 260 | loss 3.3076 | step time 14.73ms\n",
      "step 270 | loss 3.3189 | step time 16.74ms\n",
      "step 280 | loss 3.3237 | step time 16.27ms\n",
      "step 290 | loss 3.3166 | step time 15.93ms\n",
      "step 300 | loss 3.3312 | step time 9.43ms\n",
      "step 310 | loss 3.3120 | step time 14.60ms\n",
      "step 320 | loss 3.3002 | step time 15.48ms\n",
      "step 330 | loss 3.3156 | step time 12.72ms\n",
      "step 340 | loss 3.3081 | step time 13.61ms\n",
      "step 350 | loss 3.3026 | step time 9.35ms\n",
      "step 360 | loss 3.3116 | step time 14.30ms\n",
      "step 370 | loss 3.3235 | step time 15.39ms\n",
      "step 380 | loss 3.3047 | step time 9.37ms\n",
      "step 390 | loss 3.3063 | step time 12.77ms\n",
      "step 400 | loss 3.3171 | step time 16.46ms\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "hmulpybuqcqpfol\n",
      "fecjwalc\n",
      "jkzkyifbcb\n",
      "ltpnhqpspqdatkx\n",
      "lunsgocnxqpldak\n",
      "easdskhggexly\n",
      "tlzwg\n",
      "epbepyrryuphitm\n",
      "kqampeli\n",
      "xfjwkzvfucp\n",
      "--------------------------------------------------------------------------------\n",
      "step 410 | loss 3.3236 | step time 12.88ms\n",
      "step 420 | loss 3.3146 | step time 13.43ms\n",
      "step 430 | loss 3.3093 | step time 11.81ms\n",
      "step 440 | loss 3.3066 | step time 9.48ms\n",
      "step 450 | loss 3.3124 | step time 11.56ms\n",
      "step 460 | loss 3.3022 | step time 12.06ms\n",
      "step 470 | loss 3.3138 | step time 9.22ms\n",
      "step 480 | loss 3.3222 | step time 9.43ms\n",
      "step 490 | loss 3.2962 | step time 9.41ms\n",
      "step 500 | loss 3.3094 | step time 9.88ms\n",
      "step 500 train loss: 3.3113749027252197 test loss: 3.314155101776123\n",
      "test loss 3.314155101776123 is the best so far, saving model to out/model.pt\n",
      "step 510 | loss 3.2992 | step time 9.70ms\n",
      "step 520 | loss 3.3246 | step time 9.25ms\n",
      "step 530 | loss 3.3216 | step time 13.20ms\n",
      "step 540 | loss 3.3315 | step time 12.99ms\n",
      "step 550 | loss 3.3086 | step time 14.23ms\n",
      "step 560 | loss 3.3201 | step time 11.98ms\n",
      "step 570 | loss 3.3132 | step time 11.86ms\n",
      "step 580 | loss 3.3097 | step time 14.53ms\n",
      "step 590 | loss 3.3151 | step time 12.04ms\n",
      "step 600 | loss 3.3245 | step time 12.33ms\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "zcwdfaxejdagqwk\n",
      "rumuhcrgd\n",
      "zkxczyxoo\n",
      "pjtszzkvwylvtpl\n",
      "zqgg\n",
      "kzospdgbdncfffd\n",
      "vttimlnhhzjkwol\n",
      "ompikumqzfmsaw\n",
      "alvstkscgdpaqoz\n",
      "rjghdfjubzjcgjl\n",
      "--------------------------------------------------------------------------------\n",
      "step 610 | loss 3.3097 | step time 13.12ms\n",
      "step 620 | loss 3.3244 | step time 11.49ms\n",
      "step 630 | loss 3.3152 | step time 12.36ms\n",
      "step 640 | loss 3.3254 | step time 12.14ms\n",
      "step 650 | loss 3.3116 | step time 12.25ms\n",
      "step 660 | loss 3.3105 | step time 14.16ms\n",
      "step 670 | loss 3.2952 | step time 13.99ms\n",
      "step 680 | loss 3.3025 | step time 11.50ms\n",
      "step 690 | loss 3.3209 | step time 12.12ms\n",
      "step 700 | loss 3.3200 | step time 12.27ms\n",
      "step 710 | loss 3.3055 | step time 10.87ms\n",
      "step 720 | loss 3.3104 | step time 12.91ms\n",
      "step 730 | loss 3.3227 | step time 9.25ms\n",
      "step 740 | loss 3.3066 | step time 11.64ms\n",
      "step 750 | loss 3.3063 | step time 11.47ms\n",
      "step 760 | loss 3.2993 | step time 13.70ms\n",
      "step 770 | loss 3.3086 | step time 12.43ms\n",
      "step 780 | loss 3.3069 | step time 12.62ms\n",
      "step 790 | loss 3.3279 | step time 12.89ms\n",
      "step 800 | loss 3.3044 | step time 13.89ms\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "pbfgxdg\n",
      "gsycsbvrsbbmpzr\n",
      "wxel\n",
      "scdxxxagegryixy\n",
      "xnlemvtrbpeqwur\n",
      "apb\n",
      "astlyszeyqg\n",
      "ckiyka\n",
      "yy\n",
      "p\n",
      "--------------------------------------------------------------------------------\n",
      "step 810 | loss 3.3117 | step time 12.21ms\n",
      "step 820 | loss 3.3171 | step time 12.32ms\n",
      "step 830 | loss 3.3014 | step time 12.47ms\n",
      "step 840 | loss 3.3177 | step time 15.05ms\n",
      "step 850 | loss 3.3166 | step time 9.22ms\n",
      "step 860 | loss 3.3201 | step time 12.32ms\n",
      "step 870 | loss 3.2977 | step time 12.24ms\n",
      "step 880 | loss 3.3249 | step time 13.81ms\n",
      "step 890 | loss 3.3035 | step time 12.50ms\n",
      "step 900 | loss 3.3119 | step time 12.54ms\n",
      "step 910 | loss 3.3104 | step time 12.55ms\n",
      "step 920 | loss 3.3225 | step time 15.40ms\n",
      "step 930 | loss 3.2945 | step time 9.27ms\n",
      "step 940 | loss 3.3197 | step time 9.58ms\n",
      "step 950 | loss 3.3007 | step time 9.28ms\n",
      "step 960 | loss 3.2984 | step time 48.81ms\n",
      "step 970 | loss 3.3068 | step time 9.31ms\n",
      "step 980 | loss 3.2925 | step time 12.28ms\n",
      "step 990 | loss 3.3059 | step time 15.30ms\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
